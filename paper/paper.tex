\documentclass{article}

% We suggest
% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2020

% ready for submission
%\usepackage{neurips_2020_tda}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2020_tda}

% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2020_tda}

% to avoid loading the natbib package, add option nonatbib:
\usepackage[nonatbib]{neurips_2020_tda}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{booktabs}       % professional-quality tables
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{microtype}      % microtypography
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{paralist}       % in-paragraph enumerations
\usepackage{siunitx}        % SI units
\usepackage{amsmath,amssymb}
\urlstyle{same}

%HANS's CONVENIENCES
\usepackage{mymacros}


\title{Multidimensional Persistence Module Classification via Lattice-Theoretic Convolutions}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{%
  Hans Riess  \\
  Department of Electrical and Systems Engineering \\
  Universiyt of Pennsylvania\\
  Philadelphia, PA 19104 \\
  \texttt{hmr@seas.upenn.edu} \\
  % examples of more authors
  \And
  Jakob Hansen \\
  Department of Mathematics \\
  Ohio State University \\
  Columbus, OH 43210 \\
  \texttt{hansen.612@osu.edu}
  % Address \\
  % \texttt{email} \\
  % \AND
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
}

\begin{document}

\maketitle

\begin{abstract}
 To write...
\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}

Persistent homology has the ability to discern both the global topology \cite{ghrist_barcodes:_2008}
and local geometry \cite{} of finite metric spaces (e.g. embedded weighted
graphs, point clouds in $\R^d$) making it a befitting feature for the purposes of
training a neural network. Single-dimensional homological persistence has drawn
recent attention in deep learning \cite{hofer_deep_2017,pun_persistent-homology-based_2018,bruel-gabrielsson_topology_2020}. This is, in part, due to a wide range
of efficient software libraries \cite{otter_roadmap_2017,henselman_matroid_2017,bauer_ripser:_2019} for computing barcodes (in the
``west-coast'' lingo) of filtrations (e.g. Rips, alpha, {C}ech, sub-levelset).
Barcodes provide a compact shape descriptor for metric space data that
is stable with respect to Gromov-Hausdorff distance \cite{}. What's more, barcodes are encodable as features for performing various machine learning tasks (e.g.~classification). Competing recipes for featurizing barcodes from single dimensional persistence, including persistence images \cite{}, persistence landscapes \cite{}, and more exotic methods \cite{tropical}, are gaining momentum in machine learning and other scientific spheres.

Multidimensional persistence generalizes single-dimensional persistent homology in order to tackle filtrations parameterized in multiple dimensions.
Such filtrations arise, for example, in a Rips filtration of a finite metric space (i.e.~a Rips filtration) for which the finite metric space itself is parameterized by a signal attributing a vector in $\R^d$ to every point of the metric space. Restricting to points in the metric space appearing below a certain coordinate-wise threshold results in a mulit-filtration of the cooresponding Rips complexes.

Multidimensional persistence boasts stability properties \cite{lesnick_theory_2015}, if computational intractable.
While these properties are impressive, they avert a combinatorial description akin to the barcode.
This is not by accident.
In fact, no such compact description for
multidimensional persistence is possible \cite{carlsson_theory_2009}.
All hope is not lost. Families of integer-valued maps over the parameter space of a persistence module called \textit{invariants} have qualified ability to characterize multidimensional persistence.
Invariants considered in this paper are include the \textit{Hilbert function}, indicating the rank of homology at a particular point in parameter space, and the \textit{multi-graded Betti numbers}, a family of discrete algebraic invariants arising from a construction in homological algebra; the Hilbert function is nothing more than parametrized (topological) Betti numbers\footnote{\textit{Caveat lector:} the
multi-graded Betti numbers are not the same as the topological Betti numbers
$\beta_i(\mathbb{X}) = \dim(H_i(\mathbb{X}))$.}, the muilti-graded Betti numbers have their own geometric interpretation \cite{Knudson2008}.
The \textit{rank invariant}, another invariant, is shown to be complete in the case of single dimensional persistence \cite{carlsson_theory_2009}; due to it's difficulty to compute it is not considered here.

Multidimensional persistence suffers several hindrances to its usefulness in machine learning. (i) Software for computing invariants for multidimensional persistence is scarce. To the authors' knowledge, RIVET is the only available software for computing multidimensional persistence \cite{lesnick_interactive_2015}; RIVET specializes to $2$-dimensional persistence and provides a user-interface for visualizing multi-filtrations as well as a pipline for computing the Hilbert function and the multi-graded Betti numbers in degrees $0$, $1$, and $2$. (ii) For all the attention multidimensional persistence has received, there has been no activity, as far as the authors are aware, studying generating features from multidimensional persistence modules.

We hope to ignite an interest in filling both of these gaps. In this paper, we propose a naive featurization of multidimensional persistence modules based on the invariants aforementioned. We design an architecture for classifying these persistence modules. Our architecture employs a lattice-theoretic notion of convolution, thereby respecting the order relation of the parameters of the persistence module. We implement our model and compare the performance of our proposed lattice-convolutional architecture with a (simplified) standard convolutional architecture (e.g.~similar to ImageNET).

\section{Backgound}
Due to space constraints, we can offer only a brief overview of multiparameter
persistent homology. For a primer on persistent homology,
see~\cite{ghrist_barcodes:_2008,carlsson_topology_2009}; for multiparameter
persistent homology, see~\cite{}. An introduction to lattices may be found in~\cite{}. 

\subsection{Rips complexes and persistent homology}
Let $(\mathcal M,d)$ be a finite metric space. The \introduce{Vietoris-Rips
  complex} of $\mathcal M$ at scale $r$ is the abstract simplicial complex
$\text{Rips}_r(\mathcal M)$ whose simplices are subsets of $\mathcal M$ of
diameter at most $r$. There is a natural inclusion $\text{Rips}_r(\mathcal M)
\to \text{Rips}_{r'}(\mathcal M)$ for $r \leq r'$.

Applying the simplicial homology functor (with coefficients in a field $k$)
$H_i$ to $\text{Rips}_r(\mathcal M)$ produces a sequence of vector spaces
$PH_i(r)$. The inclusions $\text{Rips}_r(\mathcal M) \to
\text{Rips}_{r'}(\mathcal M)$ induce maps $PH_i(r) \to PH_i(r')$, producing the
data of \introduce{persistence module}. This structure can be compactly
described as a functor from $\R$, viewed as a category via its standard order
structure, to the category $\Vect_k$ of vector spaces over $k$. The simplicity
of the category $\R$ gives these persistence modules simple structure: they
decompose as direct sums of interval modules $I_{[a,b)}$, which have
$I_{[a,b)}(r) = k$ for $a \leq r < b$ and zero otherwise. The maps are
the identity where possible and the zero map otherwise.
% is this the right shape for the intervals?

This representation theoretic fact gives a representation\footnote{Pun intended.} of
$\R$-indexed persistence modules via \introduce{barcodes} or
\introduce{persistence diagrams}. Each bar $I_{[a,b)}$ in the barcode represents
a homology class which is \introduce{born} at $a$ and \introduce{dies} at $b$.

\subsection{Multiparameter persistence}
The Rips construction produces a filtration of simplicial complexes from a
finite metric space; it is natural to consider the behavior of the homology
functor over a pair of coherent filtrations. Consider
a finite metric space $(\mathcal M, d)$ and a filtration
function $\rho:\mathcal M \to \R$. This data specifies a bifiltration of
simplicial complexes given by
\[\mathbb{X}_{r,t} = \text{Rips}_r{x \in \mathcal M \mid \rho(x) \leq t}.\]
There is a natural inclusion $\mathbb{X}_{r,t} \hookrightarrow
\mathbb{X}_{r',t'}$ whenever $(r,t) \leq (r',t')$ in the lattice $\R \times R$.
Composing with the homology functor produces a 2-parameter persistence module
\[PH_{i}: \R_+ \times \R \to \Vect; \quad (r,t) \mapsto H_i(X_{r,t}).\]
If we assume that (i) $PH_i$ stabilizes for $r \geq R$ and $t \geq T$ for
sufficiently large $R, T$ and (ii) the induced map on homology
$H_i(\mathbb{X}_{r,t}) \to H_i(\mathbb{X}_{r',t'})$ is an isomorphism for all
but finitely many pairs $(r,t) \leq (r',t')$, then we can restrict the domain of
our persistence module $PH_i$---possibly after a reparameterization of the
filtration---to a finite order lattice $L = [m] \times [n]$, where $[n] =
\{0,1,2,\ldots,n\}$, obtaining a persistence module $M: L \to \Vect$.
% I'm not sure this is the right finiteness condition. It's very strong for an
% $\R^2$-indexed peristence module.
% we can also pull a persistence module back over an inclusion of a finite subset...
More generally, this model accepts as inputs signals on any finite lattice $L$
with features extracted from a generalized persistence module supported on $L$.

%% something about why lattice convolutions make sense to use...

While there does not exist a discrete set of invariants for $M$, we can extract
meaningful features. Two particularly informative types of features are 
 the \introduce{Hilbert function}
  \[\text{Hilb}: L \to \Z_+;\quad \mathbf{x} \mapsto \dim(M_\mathbf{x}),\] and
the \introduce{multi-graded Betti numbers} $\xi_j: L \to \Z_+$, for $j =
0,1,2$. For $M = PH_i$ as above, the Hilbert function counts the number of
connected components ($i=0$), cycles ($i=1$), or higher dimensional voids ($i >
1$) of the complex $\mathbb{X}_{r,t}$ at each $(r,t) \in L$. The multi-graded
Betti numbers, on the other hand, capture information about locations of births
and deaths of persistence classes.

\subsection{Lattice-theoretic signal processing}\label{sec:latticeconv}


\section{Lattice Convolutional Neural Networks}\label{sec:latticeCNN}

Convolutions for signals defined over $\R^n$ taken as an abelian group are
widely used in signal processing. In particular, two-dimensional convolutions
have served as an easily parameterized and efficient set of linear operations
adapted to the structure of images. Their extreme utility in computer vision
problems is owed to the translation equivariance properties of images: humans
naturally recognize an image translated via an additive reparameterization as
equivalent to the original.

The data of a multidimensional persistence module is also indexed by $\R^n$ or a
regular finite subset thereof, but its natural algebraic structure is not that
of an abelian group. Rather, with its partial order structure, the indexing set
is a lattice. In processing signals associated with the persistence module, it
may be useful to take this structure into account rather than imposing the
abelian group structure implied by standard convolutions.

To this end, we construct a lattice convolution-based neural network layer
suitable for use with features originating from multidimensional persistence
modules. We specialize the convolutions described in
Section~\ref{sec:latticeconv} to the particular case of regular finite
sublattices of $\R^2$. The meet and join operations are easily computed
elementwise:
\[(r,t) \wedge (r',t') = (\min(r,r'),\min(t,t'));\quad (r,t)\vee (r',t') =
  (\max(r,r'),\max(t,t')).\]

A lattice convolution layer takes as input an $N_{\text{in}}$-dimensional signal
$f : [m] \times [n] \to \R^{N_{\text{in}}}$ and outputs an
$N_{\text{out}}$-dimensional signal $ [m] \times [n] \to
\R^{N_{\text{out}}}$. The layer's parameters are given by a function $g: [m]
\times [n] \to \R^{N_{\text{out}}\times N_{\text{in}}}$. If we label the
entries of $f(x,y)$ by $f_i$ and the entries of $g(x,y)$ by $g^i_j$ The layer then acts by
\[\text{MeetConv}(f)(x,y)^j = \sum_{i} (f_i \ast_{\wedge} g^i_j)(x,y) = \sum_i
  \sum_{(a,b) \in [m]\times [n]} f_i(x \wedge a, y \wedge b)g^i_j(a,b)\]
in the case of convolution with respect to the meet operation, and
\[\text{JoinConv}(f)(x,y)^j = \sum_{i} (f_i \ast_{\vee} g^i_j)(x,y) = \sum_i
  \sum_{(a,b) \in [m]\times [n]} f_i(x \vee a, y \vee b)g^i_j(a,b)\]
in the case of convolution with respect to the join operation.

Convolutional neural networks are uesful in part because the convolution kernels
(here the functions $g$) can have very small support, enforcing locality and
reducing the number of parameters that must be learned. In the standard
convolutional setting, these kernels are implicitly supported in a neighborhood
of the origin, but the location of the kernel is not usually explicitly
specified. In the lattice setting, we do need to specify where the kernel
resides. Just as when we treat our domain as an abelian group, the kernel should
be supported near the identity, when we treat our domain as a lattice, the
kernel should be supported near the neutral element of the operation. That is,
for a meet convolution, $g$ should have support near the maximum $(m,n)$, and
for a join convolution, $g$ should have support near the minimum $(0,0)$.







\section{Experiments}
We use a small portion of the Princeton ModelNet dataset as a source of finite
metric spaces. This dataset consists of hundreds of 3-dimensional CAD models
representing objects from 40 classes. We select two of the classes and sample
points from the 3d models to produce finite metric spaces embedded in $\R^3$.
We then compute the corresponding multidimensional persistence modules, from
which we produce features used as an input to a convolutional neural net
classifier.

The pipline thus begins with a 3d model, which is sampled to produce a point
cloud in $\R^3$. This point cloud then produces a bifiltered simplicial complex,
whose persistent homology we calculate using RIVET~\cite{lesnick_interactive_2015}, producing
lattice-indexed signals given by the Hilbert function and the multi-graded Betti
numbers. These are then passed to the classifier, which produces a class prediction.

As the filter function on these data sets, we use
\[\rho_{\text{codense}}(x;k) = \frac{k \diam(\mathcal M)}{\sum_{y \in N_k(x)}
    d(x,y)},\]
where $N_k(x)$ is the set of the $k$ nearest neighbors to $x$. This is the
\introduce{codensity} filtration, so named because the points in the densest
regions of $\mathcal M$ will appear first. A folk theorem is that the
two-parameter persistent homology of a Rips/codensity bifiltration is stable
under non-Hausdorff perturbations: \textit{the (Rips) persistent homology of a
  point sample and another obtained by adding a small number of points at random
are close with respect to the interleaving distance}.

We compare the performance of two convolutional networks on this classification
task. One uses the lattice-convolution based layers described in
Section~\ref{sec:latticeCNN}, and the other uses standard convolutional layers.
Each has three convolutional layers followed by three fully connected layers.




\section{Discussion}
Discuss.


\bibliographystyle{plain}
\bibliography{latticenn}

\end{document}
